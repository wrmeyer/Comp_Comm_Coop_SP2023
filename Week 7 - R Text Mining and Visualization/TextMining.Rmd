---
title: "R Text Mining with Quanteda & Tidytext"
author: "Abby Youran QIN"
project: "Programming Program For Proletariat"
output:
  html_document:
    df_print: paged
---

### Installing & Importing Libraries
```{r message=FALSE}
packages <- c("tidyverse", # For data frame processing
              "quanteda", # For text processing
              "quanteda.textmodels",
              "quanteda.textstats",
              "quanteda.textplots",
              "knitr", # For html knitting
              "tidytext", # For sentiment analysis
              "ggplot2", # For visualization
              "ggrepel", # For visualizing texts with minimal overlap
              "fastDummies") # For creating dummy variables
 
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages], repos = 'http://cran.us.r-project.org')
}
invisible(lapply(packages, library, character.only = TRUE))
```

### Importing & Cleaning Files 
```{r}
setwd("/Users/abby/Documents/GitHub/ProgrammingProgramForProletariat/HelloWorld/R_2_TextMining") 
df <- read.csv("blkchain&env.csv")
summary(df)
```

```{r}
df$Date <- as.Date(df$Date)
df$Source <- as.factor(df$Source)
df$year_month <- format(as.Date(df$Date), "%Y-%m") 
# extract year and month information and add one more column called "year_month"

summary(df)
```
## Text Processing with Quanteda

* References:  
  + [Quanteda Tutorials](https://tutorials.quanteda.io/)
  + [NTNU ENC2036 Corpus Linguistics](https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/corpus-analysis-a-start.html#keyword-in-context-kwic)
  

* Structure of quanteda objects:  
  + **Corpus**: The library of original texts, retaining the structure of articles, and containing document-level variables (docvars). The best practice is to *keep the corpus as it originally is*, without recording any changes that occur during the text processing onto the corpus. 
  + **Tokens**: Texts are tokenized, and stored as vectors with *positions of words preserved*. During the tokenization process, we can remove stopwords, punctuations, numbers, urls, symbols, etc. These changes should be recorded onto the dfm object but not the corpus object. 
  + **Document-feature matrix (dfm)**: A large matrix with documents as rows, features (unique words) as columns, and different features' numbers of occurrences in different documents as cells. In a dfm, *positions of words are not preserved*.

### Corpus
**Step 1**. Creating a corpus using corpus() function. 
```{r}
corp <- corpus(df$Body, # the first parameter is the column that contain your texts
               # the docvars argument takes in variables associated with your texts (aka. metadata)
               docvars = data.frame(Date=df$Date, 
                                    Source=df$Source, 
                                    Length=df$Length, 
                                    year_month=df$year_month))
summary(corp, 5) # Check the first five rows of your corpus, by default r will output 100 rows
```
**Step 2**. Making a plot describing the general temporal trend of the articles.  

*2.1*. Getting a summary of all texts in the corpus (e.g., number of tokens and sentences in each article). 
```{r}
corp_sum <- summary (corp, n = Inf) 
# We specify n=Inf so that all texts in the corpus are summarized. 
# By default, R only summarizes the first 100 texts.
```

*2.2*. Plotting the temporal order on a line graph. 
```{r}
ggplot(corp_sum, aes(x = year_month, y = Tokens)) + 
    # year_month on the x-axis, number of tokens on the y-axis
    geom_line() +
    # generating line plot
    geom_point() +
    # marking articles with points 
    theme_bw() +
    # using a black & white theme (optional)
    theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
    # tilting the x-axis marks in 90 degrees and adjust the marks' positions
    xlab("Year-Month") # changing the x-axis label
```

From this graph, we can see more articles on blockchain and environment were published in 2022 than 2021. But we do not know who published these articles. Why don't we label the sources on the graph?  
```{r}
ggplot(corp_sum, aes(x = year_month, y = Tokens)) + 
    # year_month on the x-axis, number of tokens on the y-axis
    geom_line() +
    # generating line plot
    geom_point() +
    # marking articles with points 
    theme_bw() +
    # using a black & white theme (optional)
    xlab("Year-Month") +
    # changing the x-axis label
    theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
    # tilting the x-axis marks in 90 degrees and adjust the marks' positions
    geom_text(aes(label = Source)) # adding sources as labels for articles
```

Well... That's kinda ugly... To avoid so many overlaps between labels, we can use the geom_text_repel() function to add text labels.

```{r}
ggplot(corp_sum, aes(x = year_month, y = Tokens)) + 
    # year_month on the x-axis, number of tokens on the y-axis
    geom_line() +
    # generating line plot
    geom_point() +
    # marking articles with points 
    theme_bw() +
    # using a black & white theme (optional)
    xlab("Year-Month") +
    # changing the x-axis label
    theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
    # tilting the x-axis marks in 90 degrees and adjust the marks' positions
    geom_text_repel(aes(label = Source)) # using geom_text_repel to avoid label overlaps
```

This looks much better, but as so many articles are not labeled, we are not sure about the representativeness of the selective labels we see in the graph. To amend it, let's simply make a stacked bar chart... 

*2.3*. Making a stacked bar chart.  
```{r}
ggplot(corp_sum, aes(x = year_month, y = Tokens)) + 
  # year_month on the x-axis, number of tokens on the y-axis
  geom_col(aes(fill=Source)) +
  # creating bars for the graph, coloring bars with articles' sources
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  # tilting the x-axis marks in 90 degrees and adjust the marks' positions
  xlab("Year-Month") # changing the x-axis label
```

Now the graph looks much better! Note that stacked bar charts are not always the optimal solution. That's the reason why I am showing my trials and errors instead of directly presenting a stacked bar chart as the single correct answer. Feel free to explore different ways to present your data!  

**Step 2**. Subsetting the corpus if you need to.  
```{r}
corp_2022 <- corpus_subset(corp, Date >= "2022-01-01") 
# Only keeping documents after 2022.01.01
ndoc(corp_2022) # Check number of documents in the subset corpus

corp_by_media <- corpus_subset(corp, Source %in% c("Blockchain.News", "CryptoDaily", "Brave New Coin")) 
# Only keeping documents from these three outlets
ndoc(corp_by_media)
```

### Tokens

**Step 1**. Creating a tokens object using tokens() function. 
```{r}
toks <- tokens(corp)
print(toks, 5) # Printing the first five tokenized documents
```
**Step 2**. Checking keywords and phrases in contexts using the kwic() function.  

*2.1*. Constructing data frames of key words/phrases and their contexts.  
```{r}
# The kwic() function allows us to go through the tokens object, 
# look for the keywords we specify in a vector,
# and tell us the 10 words before and after the keywords
keywords <- kwic(toks, pattern = c("environment*", "sustain*"), window = 10) 
# We use "environment*" to match all words starting with the "environment" prefix

# The output is essentially a table, we use kable() function to visualize the table 
kable(head(keywords, 10))


# If you want to look at phrases...
keyphrases <- kwic(toks, pattern = phrase("crypto min*"), window = 10)
kable(head(keyphrases, 10))
```

*2.2*. Visualizing key word/phrases occurrences, for which we need to connect the keywords data frame to the corpus summary data frame to retrieve meta information, e.g., the text's data of publication.  
```{r}
# renaming the text index column of the corpus summary data frame in line with the keyphrases data frame
colnames(corp_sum)[1] <- "docname"

# join the keyphrases data frame and the corpus summary data frame based on the docname column
# for more information on merging data frames, see https://www.datasciencemadesimple.com/join-in-r-merge-in-r/
kp <- keyphrases %>% left_join(corp_sum, by = "docname")

# geom_bar() automatically plot the count/sum of the required variable, so we only need to specify the category we would like to plot (no need to specify x and y axes)
ggplot(kp, aes(year_month)) + 
  geom_bar(fill = "darkturquoise") + 
  # setting the color of bars
  # for more information about colors, see https://r-graph-gallery.com/ggplot2-color.html
  xlab("Year-Month") +
  ylab("Occurrence of \"Crypto Mining\"") + # using \ escape sign to add quotation marks in character strings
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25))
```

*2.3*. Visualizing key word/phrases occurrence rate. Now we know the key phrases' occurrence in each month, but how frequent is it relative to the total tokens published in that month?  
```{r}
# First, we need to create a new data frame of the number of keyphrase occurrences in each month
month_sum <- kp %>% # taking the newly created kp data frame
  group_by(year_month) %>% # grouping entries by the year_month variable
  summarize(occurrence = n()) 
  # for each group, count how many entries are there, 
  # and record the count into the occurrence column

# Second, we need to calculate the total number of tokens published each month
month_total <- corp_sum %>% # taking the old corpus summary data frame
  group_by(year_month) %>% # grouping entries by the year_month variable
  summarize(total = sum(Tokens))
  # for each group, sum up the token numbers of all entries, 
  # and record the sum into the total column

# Third, we will join the keyphrases occurrences data frame and the total tokens data frame based on the year_month column
month_rate <- month_sum %>% left_join(month_total, by = "year_month") 
# and calculate the rate of keyword occurrences relative to the total number of tokens
month_rate$rate <- month_rate$occurrence/month_rate$total

# Finally, we can plot the occurrence rate!
ggplot(month_rate, aes(x = year_month, y = rate)) + 
    # year_month on the x-axis, occurrence rate on the y-axis
    geom_col(aes(fill = rate)) +
    # adding bars and having their colors filled based on occurrence rate
    xlab("Year-Month") +
    # changing the x-axis label
    ylab("Rate of \"Crypto Mining\" Occurrences") + 
    # changing the y-axis label
    labs(fill = "Occurrence Rate") +
    # changing legend label
    theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) 

```

**Step 3**. Removing punctuation, numbers, URLs, etc.  
```{r}
toks_nostop <- tokens(corp, 
                      remove_punct = TRUE, 
                      remove_numbers = TRUE, 
                      remove_url = TRUE, 
                      remove_symbols = TRUE)  %>%
  tokens_tolower(keep_acronyms = FALSE) %>% # turn everything into lower case
  # if keep_acronyms = TRUE, all-uppercase words will not be lowercased
  tokens_remove(pattern = stopwords("en")) # removing stopwords defined by quanteda
print(toks_nostop, 5)
```


### Dfm

**Step 1**. Constructing a dfm object using the dfm() function and inspecting. 
```{r}
dfm <- dfm(toks_nostop) # constructing a dfm object from a tokens object
print(dfm)

# checking the no. of documents and features
print(c("Number of documents:", ndoc(dfm)))
print(c("Number of features:", nfeat(dfm)))
```
**Step 2**. Checking top features and creating your customized stopwords list.  
While you are inspecting top words, open a .txt file, and type in words you identify as stopwords, separate words in different lines. Aka., type a word, click the return button, type another word, return, etc.  
```{r}
topfeatures(dfm, 10)
```

**Step 3**. Removing customized stopwords
```{r}
myStopwords <- readLines("customStop.txt", encoding = "UTF-8") # defining customized stopwords
multiword <- c("hong kong") # connecting multi-word phrases

toks_nostop <- toks_nostop %>% 
  tokens_remove(myStopwords) %>% # updating the tokens object by removing customized stopwords
  tokens_compound(pattern = phrase(multiword)) # updating the tokens object by connecting multiwords

dfm <- dfm(toks_nostop) # creating a dfm object with the updated tokens object
topfeatures(dfm, 10)
```

**Step 4**. Frequency analysis

*4.1*. Wordcloud. 
```{r}
textplot_wordcloud(dfm, max_words = 100)
```

*4.2*. Compare high frequency words by categories. 
```{r}
# For each year_month, find the top 3 words in terms of frequency
freq_by_month <- textstat_frequency(dfm, n = 1, groups = year_month)
kable(freq_by_month)

# Group dfm by a variable
dfm_by_source <- tokens(corp_by_media, 
               remove_punct = TRUE, 
               remove_numbers = TRUE, 
               remove_url = TRUE, 
               remove_symbols = TRUE)  %>%
  tokens_tolower(keep_acronyms = FALSE) %>% 
  tokens_remove(pattern = stopwords("en")) %>%
  tokens_remove(myStopwords) %>% 
  tokens_compound(pattern = phrase(multiword)) %>%
  dfm() %>%
  dfm_group(groups = Source)

# Plot grouped wordcloud
textplot_wordcloud(dfm_by_source, comparison = TRUE, max_words = 100)
```

**Step 5**. Compare relative frequency (keyness). 
```{r}
# Compare keywords' keyness
keyness_by_year <- textstat_keyness(dfm, target = dfm$Date >= "2022-01-01")
textplot_keyness(keyness_by_year, margin = 0.1, labelsize = 2.5)
```

## Sentiment Analysis with Tidytext

Reference: [Text Mining with R](https://www.tidytextmining.com/sentiment.html). 

Sentiment analysis is an interesting way to learn more about your data. There are different ways to analyze sentiments. Today, we are going to discuss the easiest one - dictionary-based sentiment analysis.  

The tidytext package has some built-in sentiment dictionaries, including [AFINN](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html), [Bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and [NRC](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). Let's have a look!  
**Note that all the three dictionaries are unigram, meaning that they only consider the sentiment of single words. "not good" will have the same sentiment score as "good"**. 

To use these dictionaries, we must first tokenize our texts in a dataframe.  
```{r}
# Using tidytext's unnest_tokens() function to unnest words in df's Body column into a column called "word"
df_tokens <- df %>% unnest_tokens(word, Body)
```

### AFINN
The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.  
Let's take a look!
```{r}
get_sentiments("afinn")
```

Now let's try to use this dictionary!
```{r}
afinn <- get_sentiments("afinn") # put the dictionary into a data frame

# matching every word that exists in both our news data frame and the afinn dictionary
df_afinn <- df_tokens %>% inner_join(afinn, by = "word") 


afinn_sentiment_by_month <- df_afinn %>%
  group_by(year_month) %>% # grouping words by month
  summarise(sentiment = sum(value)) # for each month, summing up the values of all its words

ggplot(afinn_sentiment_by_month, aes(x = year_month, y = sentiment)) +
  geom_col() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Year-Month") +
  ylab("Sentiment Score")
```

Yay! It worked!!! But wait... Isn't the graph a bit familiar? Well, it seems that we get higher sentiment score in the later months just because we have more articles (and therefore, more tokens) in those months... To put things into perspective, we need to calculate the sentiment score in proportion to the total number of tokens.  

Let's try one more time!

```{r}
# Do you remember the month_total dataframe we made to calculate our keywords' occurrence rate? 

month_total <- corp_sum %>% # taking the old corpus summary data frame
  group_by(year_month) %>% # grouping entries by the year_month variable
  summarize(total = sum(Tokens))
  # for each group, sum up the token numbers of all entries, 
  # and record the sum into the total column


# Adding each month's total token number to the sentiment_by_month data frame
afinn_sentiment_by_month <- afinn_sentiment_by_month %>% 
  inner_join(month_total, by = "year_month") 
afinn_sentiment_by_month$rate <- afinn_sentiment_by_month$sentiment/afinn_sentiment_by_month$total

ggplot(afinn_sentiment_by_month, aes(x = year_month, y = rate)) +
  geom_col() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Year-Month") +
  ylab("Sentiment Score")
```

Now it looks much nicer! Let's proceed to the next dictionary!

### Bing
The Bing lexicon categorizes words in a binary fashion into positive and negative categories.  
```{r}
get_sentiments("bing")
```

```{r}
bing <- get_sentiments("bing") # put the dictionary into a data frame

# matching every word that exists in both our news data frame and the bing dictionary
df_bing <- df_tokens %>% inner_join(bing, by = "word") 


bing_sentiment_by_month <- df_bing %>% 
  group_by(year_month) %>% # grouping the words based on months
  count(sentiment) %>% # count the positive and negative words for each month
  pivot_wider(names_from = sentiment, values_from = n) # put positive and negative counts into different columns

# calculating each month's overall sentiment score by minus the negative score from the positive score
bing_sentiment_by_month$sentiment <- bing_sentiment_by_month$positive - bing_sentiment_by_month$negative

# Adding each month's total token number to the sentiment_by_month data frame
bing_sentiment_by_month <- bing_sentiment_by_month %>% 
  inner_join(month_total, by = "year_month") 
bing_sentiment_by_month$rate <- bing_sentiment_by_month$sentiment/bing_sentiment_by_month$total


ggplot(bing_sentiment_by_month, aes(x = year_month, y = rate)) +
  geom_col() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Year-Month") +
  ylab("Sentiment Score")
```

### NRC
nrc: The nrc lexicon categorizes words into one or more categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.
```{r}
get_sentiments("nrc")
```

```{r}
nrc <- get_sentiments("nrc") # put the dictionary into a data frame

# matching every word that exists in both our news data frame and the nrc dictionary
df_nrc <- df_tokens %>% inner_join(nrc, by = "word") 

df_nrc <- dummy_cols(df_nrc, select_columns = "sentiment")
# using the dummy_cols() function from the "fastDummies" package to create dummy variables based on the sentiment column

nrc_sentiment_by_month <- df_nrc %>% 
  group_by(year_month) %>% # grouping the words based on months
  # For each month, summing up the occurrences of words that are positive, negative, etc.
  # I ranked them according to my own subjective feeling of their positiveness and negativeness
  summarize(a_joy = sum(sentiment_joy),
            b_trust = sum(sentiment_trust),
            c_positive = sum(sentiment_positive),
            d_anticipation = sum(sentiment_anticipation),
            e_surprise = sum(sentiment_surprise),
            f_fear = sum(sentiment_fear),
            g_negative = sum(sentiment_negative),
            h_sadness = sum(sentiment_sadness),
            i_disgust = sum(sentiment_disgust),
            j_anger = sum(sentiment_anger))

# Adding each month's total token number to the sentiment_by_month data frame
nrc_sentiment_by_month <- nrc_sentiment_by_month %>% inner_join(month_total, by = "year_month")

# Divide each sentiment score by the total number of tokens
nrc_sentiment_by_month[,2:11] <- nrc_sentiment_by_month[,2:11]/nrc_sentiment_by_month$total

nrc_sentiment_by_month %>% 
  # regarding each cell (except those in the year_month and total columns) as a key-value pair,
  # with the key being its column name (sentiment type) and the value being its value
  gather(key, value, -c(year_month, total)) %>% 
  
  # plotting year_month on the x-axis, and the value of each sentiment type in that month on the y-axis
  ggplot(aes(x = year_month, y = value)) +
  
  # coloring the bars based on the sentiment category
  geom_col(aes(fill = key)) +
  
  # customizing a color scheme for different sentiments
  scale_fill_manual(values = c("a_joy" = "#257A24",
                               "b_trust" = "#3C8321",
                               "c_positive" = "#5A8C1E",
                               "d_anticipation" = "#7E951A",
                               "e_surprise" = "#9E9315",
                               "f_fear" = "#A38413",
                               "g_negative" = "#A87310",
                               "h_sadness" = "#B14A0A",
                               "i_disgust" = "#B53307",
                               "j_anger" = "#BF0003")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Year-Month") +
  ylab("Sentiment Score") +
  labs(fill = "Sentiment Types")
```

What if I want to see different outlets' sentiments?
```{r}
nrc_sentiment_by_month_source <- df_nrc %>% 
  group_by(year_month, Source) %>% # grouping the words based on months and sources
  
  # For each month-Source pair, summing up the occurrences of words that are positive, negative, etc.
  # I ranked them according to my own subjective feeling of their positiveness and negativeness
  summarize(a_joy = sum(sentiment_joy),
            b_trust = sum(sentiment_trust),
            c_positive = sum(sentiment_positive),
            d_anticipation = sum(sentiment_anticipation),
            e_surprise = sum(sentiment_surprise),
            f_fear = sum(sentiment_fear),
            g_negative = sum(sentiment_negative),
            h_sadness = sum(sentiment_sadness),
            i_disgust = sum(sentiment_disgust),
            j_anger = sum(sentiment_anger))


month_source <- corp_sum %>% # taking the old corpus summary data frame
  group_by(year_month, Source) %>% # grouping entries by the year_month and source variables
  summarize(total = sum(Tokens))
  # for each group, sum up the token numbers of all entries, 
  # and record the sum into the total column

nrc_sentiment_by_month_source <- nrc_sentiment_by_month_source %>% 
  inner_join(month_source, by = c("year_month", "Source"))

# Divide each sentiment score by the total number of tokens
nrc_sentiment_by_month_source[,3:12] <- nrc_sentiment_by_month_source[,3:12]/nrc_sentiment_by_month_source$total



nrc_sentiment_by_month_source %>% 
  
  # regarding each cell (except those in the year_month and Source columns) as a key-value pair,
  # with the key being its column name (sentiment type) and the value being its value
  gather(key, value, -c(year_month, Source, total)) %>% 
  
  # plotting year_month on the x-axis, and the value of each sentiment type in that month on the y-axis
  ggplot(aes(x = year_month, y = value)) +
  
  # coloring the bars based on the sentiment category
  geom_col(aes(fill = key)) +
  
  # present each source in one facet, make three rows of facets
  facet_wrap(~Source, nrow = 3) +
  
  # customizing a color scheme for different sentiments
  scale_fill_manual(values = c("a_joy" = "#257A24",
                               "b_trust" = "#3C8321",
                               "c_positive" = "#5A8C1E",
                               "d_anticipation" = "#7E951A",
                               "e_surprise" = "#9E9315",
                               "f_fear" = "#A38413",
                               "g_negative" = "#A87310",
                               "h_sadness" = "#B14A0A",
                               "i_disgust" = "#B53307",
                               "j_anger" = "#BF0003")) +
  xlab("Year-Month") +
  ylab("Sentiment Score") +
  labs(fill = "Sentiment Type") +
  
  # getting rid of x-axis ticks, texts, and label for a pleasing visualization
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

### Comparing Sentiment Dictionaries
```{r}
head(afinn_sentiment_by_month)
head(bing_sentiment_by_month)
head(nrc_sentiment_by_month)

# calculating each month's overall nrc sentiment score by minus the negative score from the positive score
nrc_sentiment_by_month$rate = nrc_sentiment_by_month$c_positive - nrc_sentiment_by_month$g_negative

# processing each dataframe by adding a dictionary column to note down the respective dictionary, and
# getting rid of unnecessary columns
afinn_sentiment <- afinn_sentiment_by_month[c("year_month", "rate")] %>% mutate(dict = "AFINN")
bing_sentiment <- bing_sentiment_by_month[c("year_month", "rate")] %>% mutate(dict = "Bing")
nrc_sentiment <- nrc_sentiment_by_month[c("year_month", "rate")] %>% mutate(dict = "NRC")

# combining the three data frames
sentiment_compare <- rbind(afinn_sentiment, bing_sentiment, nrc_sentiment)

ggplot(sentiment_compare, aes(x = year_month, y = rate, fill = dict)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ dict, ncol = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Year-Month") +
  ylab("Sentiment Score") +
  labs(fill = "Sentiment Dictionaries")
```


